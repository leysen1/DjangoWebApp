{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyphen #for syllables\n",
    "from textblob import TextBlob # for subjectivity \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get word2vec from a corpus\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "import nltk as nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import Ridge,LinearRegression,BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    df_file = pd.read_table(file_name, header = None, index_col=2,encoding = \"ISO-8859-1\")\n",
    "    df_file.columns = ['Score','other_scores','text']\n",
    "    df_file = df_file.drop('other_scores', axis = 'columns')\n",
    "    df_file['text'] = df_file['text'].astype('str')\n",
    "    return df_file\n",
    "\n",
    "df_email = read_file('email')\n",
    "df_news = read_file('news')\n",
    "df_news['len'] = df_news.text.apply(len)\n",
    "df_news = df_news[df_news['len'] < 500]\n",
    "df_news = df_news.drop('len', axis = 'columns')\n",
    "\n",
    "\n",
    "df_blog = read_file('blog')\n",
    "df_answers = read_file('answers')\n",
    "\n",
    "def create_words(temp):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = [tokenizer.tokenize(i) for i in temp.values.tolist()]\n",
    "    flat = [item.lower() for sublist in tokens for item in sublist]\n",
    "    return flat\n",
    "def create_word2vec(words):\n",
    "    vecs = Word2Vec(words)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case\n",
    "## Number of entirely capitalized words\n",
    "def entirely_capitalized_num_of_words(temp):\n",
    "    return sum([1 if i.isupper() == True else 0 for i in temp.split(' ')])\n",
    "\n",
    "## binary indicator whether a sentence is entirely capitalized\n",
    "def entirely_capitalized_sentence(temp):\n",
    "    if entirely_capitalized_num_of_words == len([i for i in temp.split()]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "## binary indicator whether the first word is capitalized\n",
    "def first_word_capitalized(temp):\n",
    "    if temp[0].isupper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency (still outstanding!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity\n",
    "## One-hot features for entity types (e.g. PERSON, LOCATION) occurring in the sentence\n",
    "def count_nes(temp):\n",
    "    tokens = word_tokenize(temp)\n",
    "    tags = pos_tag(tokens)\n",
    "    chunked = ne_chunk(tags)\n",
    "    named_entities = ['DATE','FACILITY','GPE', 'LOCATION', 'MONEY', 'ORGANIZATION','PERCENT','PERSON','TIME']\n",
    "    counts ={}\n",
    "    for ne in named_entities:\n",
    "        counts[ne] = 0\n",
    "    for tree in chunked.subtrees():\n",
    "        for named_entity in named_entities:\n",
    "            if tree.label() == named_entity:\n",
    "                counts[named_entity] = counts[named_entity] + 1\n",
    "    return counts\n",
    "## average length, in characters, of PERSON mentions\n",
    "def aver_length_person(temp):\n",
    "    tokens = word_tokenize(temp)\n",
    "    tags = pos_tag(tokens)\n",
    "    chunked = ne_chunk(tags)\n",
    "    word = ''\n",
    "    for tree in chunked.subtrees():\n",
    "        if tree.label() == 'PERSON':\n",
    "            for i in range(len(tree)):\n",
    "                word = word + tree[i][0]\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical\n",
    "## Number of contractions in the sentence, normalized by length\n",
    "def contractions_norm(temp):\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    count = 0\n",
    "    tokens = [i for i in temp.split()]\n",
    "    tokens = [i.lower() for i in tokens]\n",
    "    for word in tokens:\n",
    "        if word in contractions.keys():\n",
    "            count = count + 1\n",
    "    return count / len(tokens)\n",
    "\n",
    "## average word length\n",
    "def aver_word_length(temp):\n",
    "    tokens = word_tokenize(temp)\n",
    "    lengths = []\n",
    "    for word in tokens:\n",
    "        lengths.append(len(word))\n",
    "    return sum(lengths) / len(lengths)\n",
    "\n",
    "## average word log-frequency according to Google Ngram corpus (used brown corpus and train set wordsinstead)\n",
    "def aver_log_freq(temp,frequencies):\n",
    "    freqlist = []\n",
    "    tokens = [i.lower() for i in word_tokenize(temp)]\n",
    "    for word in tokens:\n",
    "        freqlist.append(np.log1p(frequencies[word]))\n",
    "    return sum(freqlist) / len(freqlist)\n",
    "\n",
    "## Formality score (to come from Pavlich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams\n",
    "def word_grams(temp, min=1, max=4):\n",
    "    temp = temp.lower()\n",
    "    temp = temp.split(' ')\n",
    "    s = []\n",
    "    for n in range(min, max):\n",
    "        for ngram in ngrams(temp, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "## Number of occurrences of each POS tag, normalized by the sentence length.\n",
    "\n",
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "def postag_occurences(temp):\n",
    "    counts = {}\n",
    "    tokens = word_tokenize(temp)\n",
    "    length = len(tokens)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    freq = FreqDist([y for (x,y) in pos_tagged])\n",
    "    elements = [i for i in freq.elements()]\n",
    "    for key in tagdict.keys():\n",
    "        counts[key] = 0\n",
    "        for element in elements:\n",
    "            counts[element] = freq[element] / length\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation\n",
    "## Number of ‘?’, ‘...’, and ‘!’ in the sentence\n",
    "def count_punct(temp):\n",
    "    tokens = word_tokenize(temp)\n",
    "    counts = {}\n",
    "    counts['...'] = 0\n",
    "    counts['!'] = 0\n",
    "    counts['?'] = 0\n",
    "    for token in tokens:\n",
    "        if token in counts.keys():\n",
    "            counts[token] = counts[token] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readability\n",
    "## Length of the sentence, in words and characters; Flesch-Kincaid Grade Level score\n",
    "def num_of_words(temp):\n",
    "    tokens = word_tokenize(temp)\n",
    "    return len(tokens)\n",
    "def num_of_chars(temp):\n",
    "    return(len(temp))\n",
    "\n",
    "def count_syllables(temp):\n",
    "    dic = pyphen.Pyphen(lang ='en')\n",
    "    return (len(dic.inserted(temp).split('-')))\n",
    "\n",
    "def kinclaid_score(temp):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    total_words = len(tokenizer.tokenize(temp))\n",
    "    total_sentence = 1\n",
    "    total_syllables = sum([count_syllables(word) for word in tokenizer.tokenize(temp)])\n",
    "    count_syllables(temp)\n",
    "\n",
    "    score = 0.39 * ((total_words / total_sentence)) + 11.8*((total_syllables / total_words)) - 15.59\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity (all normalized by sentence length)\n",
    "## Number of passive constructions (outstanding)\n",
    "\n",
    "## number of hedge words (fillers added)\n",
    "def count_hedge_words(temp):\n",
    "    hedge_words = pd.read_table('hedge_words.txt',header = None)\n",
    "    hedge_words = hedge_words[0].values.tolist()\n",
    "    words = word_tokenize(temp)\n",
    "    counts = 0\n",
    "    for word in words:\n",
    "        if word.lower() in hedge_words:\n",
    "            counts = counts + 1\n",
    "    return counts\n",
    "\n",
    "def count_filler_words(temp):\n",
    "    hedge_words = pd.read_table('filler_words.txt',header = None)\n",
    "    hedge_words = hedge_words[0].values.tolist()\n",
    "    words = word_tokenize(temp)\n",
    "    counts = 0\n",
    "    for word in words:\n",
    "        if word.lower() in hedge_words:\n",
    "            counts = counts + 1\n",
    "    return counts\n",
    "\n",
    "## number of 1st person pronouns\n",
    "def num_of_firstperson(temp):\n",
    "    list_of_pronouns = ['i','me','mine','my','we','us','our','ours']\n",
    "    words = word_tokenize(temp)\n",
    "    words = [word.lower() for word in words]\n",
    "    counts = 0\n",
    "    for word in words:\n",
    "        if word in list_of_pronouns:\n",
    "            counts = counts + 1\n",
    "    return counts\n",
    "\n",
    "## number of 3rd person pronouns\n",
    "def num_of_thirdperson(temp):\n",
    "    list_of_pronouns = ['he','she','it','him','her','its', 'hers', 'they','them','their','theirs']\n",
    "    words = word_tokenize(temp)\n",
    "    words = [word.lower() for word in words]\n",
    "    counts = 0\n",
    "    for word in words:\n",
    "        if word in list_of_pronouns:\n",
    "            counts = counts + 1\n",
    "    return counts\n",
    "\n",
    "## subjectivity according to the TextBlob sentiment module\n",
    "def subjectivity(temp):\n",
    "    return TextBlob(temp).sentiment.subjectivity\n",
    "## binary indicator for whether the sentiment is positive or negative, according to the TextBlob sentiment module\n",
    "def sentiment(temp):\n",
    "    if TextBlob(temp).sentiment.polarity >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "## Average of word vectors using pre-trained word2vec embeddings, skipping OOV words.\n",
    "# Get word2vec from a corpus\n",
    "other_sents = df_news.text.apply(str.lower)\n",
    "# sents = [[i] for i in sents]\n",
    "all_sents = [word_tokenize(i) for i in other_sents]\n",
    "\n",
    "brown_sentslower = [[word.lower() for word in element]\n",
    "                    for element in brown.sents()]\n",
    "\n",
    "all_sents = brown_sentslower + all_sents\n",
    "\n",
    "vec_model = create_word2vec(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordvector(temp):\n",
    "    temp = temp.lower()\n",
    "    temp = word_tokenize(temp)\n",
    "    vecs = np.zeros((100,))\n",
    "    for word in temp:\n",
    "        if word in vec_model.wv:\n",
    "            vecs = vecs + vec_model[word]\n",
    "        else:\n",
    "            vecs = vecs + np.empty((100,))\n",
    "    return vecs / len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATAFRAME FOR FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical analysis (create frequencies)\n",
    "lowered = df_news.text.apply(str.lower)\n",
    "\n",
    "all_words_1 = ''\n",
    "for i in lowered.values.tolist():\n",
    "    all_words_1 = all_words_1 + i\n",
    "\n",
    "frequencies = FreqDist(all_words_1.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_news.text.apply(wordvector)\n",
    "\n",
    "# Prepare data\n",
    "series = pd.Series(new_df.values)\n",
    "\n",
    "temp = pd.DataFrame(series.values.tolist(),index=df_news.index)\n",
    "\n",
    "from sklearn.preprocessing import robust_scale,RobustScaler\n",
    "scaler = RobustScaler(quantile_range=(49.0,51.0))\n",
    "\n",
    "scaler = scaler.fit(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(temp):\n",
    "    transformed = []\n",
    "    transformed.extend([(entirely_capitalized_num_of_words(temp))])\n",
    "    transformed.extend([(entirely_capitalized_sentence(temp))])\n",
    "    transformed.extend([(first_word_capitalized(temp))])\n",
    "    transformed.extend(list(count_nes(temp).values()))\n",
    "    transformed.extend([contractions_norm(temp)])\n",
    "    transformed.extend([aver_word_length(temp)])\n",
    "    transformed.extend([aver_log_freq(temp.lower(), frequencies)])\n",
    "    transformed.extend(list(postag_occurences(temp).values()))\n",
    "    transformed.extend(list(count_punct(temp).values()))\n",
    "    transformed.extend([num_of_words(temp)])\n",
    "    transformed.extend([num_of_chars(temp)])\n",
    "    transformed.extend([count_syllables(temp)])\n",
    "    transformed.extend([kinclaid_score(temp)])\n",
    "    transformed.extend([count_hedge_words(temp)])\n",
    "    transformed.extend([count_filler_words(temp)])\n",
    "    transformed.extend([num_of_firstperson(temp)])\n",
    "    transformed.extend([num_of_thirdperson(temp)])\n",
    "    transformed.extend([subjectivity(temp)])\n",
    "    transformed.extend([sentiment(temp)])\n",
    "    transformed.extend([item for sublist in scaler.transform(np.reshape(wordvector(temp),(1,100))).tolist() for item in sublist])\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_news.text.apply(transformation)\n",
    "\n",
    "X= np.array(list(X.values))\n",
    "\n",
    "y = df_news.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for parameters\n",
    "alphas = [10**(i) for i in range(5)]\n",
    "\n",
    "grid_scores = {}\n",
    "for alpha in alphas:\n",
    "    clf = Ridge(alpha = alpha)\n",
    "    score = np.mean(cross_val_score(clf,X,y,scoring='r2',cv = 20))\n",
    "    grid_scores[str(alpha)] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': -0.23046058609467698,\n",
       " '10': -0.17545413682623473,\n",
       " '100': -0.287626780878671,\n",
       " '1000': -0.8792559520988419,\n",
       " '10000': -1.3206618248535786}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a255d6128>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = Ridge(normalize=True,alpha = 1)\n",
    "clf.fit(X,y);\n",
    "preds = clf.predict(X)\n",
    "preds = ((preds - np.mean(y)) / (np.std(y)))\n",
    "plt.scatter(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_score(temp,clf):\n",
    "    temp = transformation(temp)\n",
    "    temp = np.reshape(temp,(1,173))\n",
    "    pred = clf.predict(temp)\n",
    "    p_mean = np.mean(clf.predict(X))\n",
    "    p_std = np.std(clf.predict(X))\n",
    "    p_z = (pred - p_mean) / p_std\n",
    "    p_final = (p_z) * np.std(y) + np.mean(y)\n",
    "    return p_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57262581])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change test word and test the model.\n",
    "test = 'I can not say that this is acceptable.'\n",
    "\n",
    "get_model_score(test,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
